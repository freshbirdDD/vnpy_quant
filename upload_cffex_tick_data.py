"""
import_cffex_tick_data_fixed.py
vn.py 4.2ç‰ˆæœ¬ - å°†åŒ…å«å¤šä¸ªCFFEXåˆçº¦çš„Tickæ•°æ®CSVå¯¼å…¥æ•°æ®åº“

ä¿®å¤ç‰ˆï¼šæ”¯æŒå•æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹æ‰¹é‡å¯¼å…¥
"""
import pandas as pd
import numpy as np
from datetime import datetime, time
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from vnpy.trader.constant import Exchange, Direction, Offset
from vnpy.trader.object import TickData
from vnpy.trader.database import BaseDatabase, get_database


class CFFEXTickDataImporterFixed:
    """CFFEXäº¤æ˜“æ‰€å¤šåˆçº¦Tickæ•°æ®å¯¼å…¥å™¨ (ä¿®å¤ç‰ˆ)"""

    # CSVåˆ—ååˆ°TickDataå±æ€§åçš„æ˜ å°„
    TICK_FIELDS = {
        # å¿…éœ€å­—æ®µ
        'UpdateTime': 'datetime',           # æ˜ å°„åˆ°TickData.datetime (éœ€è¦è½¬æ¢ä¸ºdatetimeå¯¹è±¡)
        'InstrumentID': 'symbol',           # æ˜ å°„åˆ°TickData.symbol

        # ä»·æ ¼å’Œæˆäº¤é‡å­—æ®µ
        'LastPrice': 'last_price',
        'Volume': 'volume',
        'Turnover': 'turnover',
        'OpenInterest': 'open_interest',

        # ä¹°å–ç›˜å£å­—æ®µ
        'BidPrice1': 'bid_price_1',
        'BidVolume1': 'bid_volume_1',
        'AskPrice1': 'ask_price_1',
        'AskVolume1': 'ask_volume_1',

        # å…¶ä»–ä»·æ ¼æ¡£ä½
        'BidPrice2': 'bid_price_2',
        'BidVolume2': 'bid_volume_2',
        'AskPrice2': 'ask_price_2',
        'AskVolume2': 'ask_volume_2',

        'BidPrice3': 'bid_price_3',
        'BidVolume3': 'bid_volume_3',
        'AskPrice3': 'ask_price_3',
        'AskVolume3': 'ask_volume_3',

        'BidPrice4': 'bid_price_4',
        'BidVolume4': 'bid_volume_4',
        'AskPrice4': 'ask_price_4',
        'AskVolume4': 'ask_volume_4',

        'BidPrice5': 'bid_price_5',
        'BidVolume5': 'bid_volume_5',
        'AskPrice5': 'ask_price_5',
        'AskVolume5': 'ask_volume_5',

        # å…¶ä»–ä»·æ ¼å­—æ®µ
        'UpperLimitPrice': 'limit_up',
        'LowerLimitPrice': 'limit_down',
        'PreClosePrice': 'pre_close',
        # ToDo æš‚æ—¶å…ˆæ³¨é‡Šæ‰ï¼ŒDB tick dataé‡Œæ²¡æœ‰è¿™ä¸ªå­—æ®µ
        # 'PreSettlementPrice': 'pre_settlement',
        'OpenPrice': 'open_price',
        'HighPrice': 'high_price',
        'LowPrice': 'low_price',
        'SettlementPrice': 'settlement_price',
    }

    def __init__(self):
        """åˆå§‹åŒ–å¯¼å…¥å™¨"""
        self.exchange = Exchange.CFFEX
        self.gateway_name = "TICK_CSV_IMPORT"
        self.database: BaseDatabase = get_database()

    def parse_datetime(self, dt_str: str) -> Optional[datetime]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        if pd.isna(dt_str) or not isinstance(dt_str, str):
            return None

        dt_str = str(dt_str).strip()
        if not dt_str:
            return None

        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
        date_formats = [
            '%Y-%m-%d %H:%M:%S.%f',
            '%Y/%m/%d %H:%M:%S.%f',
            '%Y%m%d %H:%M:%S.%f',
            '%Y-%m-%d %H:%M:%S',
            '%Y/%m/%d %H:%M:%S',
            '%Y%m%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y/%m/%d %H:%M',
            '%Y%m%d %H:%M',
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(dt_str, fmt)
            except ValueError:
                continue

        # å¦‚æœéƒ½ä¸æˆåŠŸï¼Œå°è¯•åªè§£ææ—¥æœŸéƒ¨åˆ†
        try:
            date_part = dt_str.split()[0]
            dt = datetime.strptime(date_part, '%Y-%m-%d')
            return dt.replace(hour=9, minute=30, second=0, microsecond=0)
        except:
            return None

    def validate_symbol(self, symbol: str) -> Optional[str]:
        """éªŒè¯å¹¶æ¸…ç†åˆçº¦ä»£ç """
        if pd.isna(symbol) or not isinstance(symbol, str):
            return None

        symbol = str(symbol).strip().upper()

        # ç§»é™¤å¯èƒ½åŒ…å«çš„äº¤æ˜“æ‰€åç¼€
        if '.' in symbol:
            parts = symbol.split('.')
            symbol = parts[0]  # å–ç¬¬ä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºåˆçº¦ä»£ç 

        # éªŒè¯åŸºæœ¬æ ¼å¼ï¼ˆè‡³å°‘2ä¸ªå­—æ¯+æ•°å­—ï¼‰
        if len(symbol) >= 2 and symbol[:2].isalpha():
            return symbol
        return None

    def parse_row_to_tick(self, row: pd.Series, index: int, field_mapping: Dict) -> Optional[TickData]:
        """
        å°†ä¸€è¡Œæ•°æ®è§£æä¸ºTickDataå¯¹è±¡
        æ³¨æ„ï¼šå¦‚æœæ—¶é—´åˆ—ä¸ºç©ºï¼Œä¼šè¿”å›Noneï¼Œè¯¥æ¡tickæ•°æ®ä¸ä¼šè¢«ä¸Šä¼ 
        """
        try:
            # 1. è§£æåˆçº¦ä»£ç å’Œæ—¶é—´ï¼ˆå¿…éœ€å­—æ®µï¼‰
            symbol = self.validate_symbol(row.get('InstrumentID'))
            if not symbol:
                return None

            # 2. è§£ææ—¶é—´ - å¦‚æœä¸ºç©ºæˆ–æ— æ•ˆï¼Œè¿”å›Noneï¼Œè¯¥æ¡tickä¸ä¸Šä¼ 
            raw_time = row.get('UpdateTime')
            dt = self.parse_datetime(raw_time)
            if not dt:  # æ—¶é—´åˆ—ä¸ºç©ºï¼Œè¿”å›Noneï¼Œä¸ä¸Šä¼ è¯¥æ¡tick
                return None

            # 3. åˆ›å»ºTickDataå¯¹è±¡
            tick = TickData(
                gateway_name=self.gateway_name,
                symbol=symbol,
                exchange=self.exchange,
                datetime=dt,
                name="",
            )

            # 4. ä½¿ç”¨æ˜ å°„å…³ç³»è®¾ç½®æ‰€æœ‰å­—æ®µ
            for csv_col, tick_attr in field_mapping.items():
                if csv_col not in row:
                    continue

                value = row[csv_col]
                if pd.isna(value) or (isinstance(value, (int, float)) and value == 0):
                    continue

                try:
                    float_value = float(value)
                    setattr(tick, tick_attr, float_value)
                except (ValueError, TypeError):
                    pass

            # 5. æ£€æŸ¥å¿…éœ€çš„ä»·æ ¼å­—æ®µ
            if not tick.last_price or tick.last_price == 0:
                if tick.bid_price_1 and tick.bid_price_1 > 0:
                    tick.last_price = tick.bid_price_1
                elif tick.ask_price_1 and tick.ask_price_1 > 0:
                    tick.last_price = tick.ask_price_1
                else:
                    return None

            # 6. ç¡®ä¿ä¹°å–ç›˜å£æœ‰æœ‰æ•ˆå€¼
            if not tick.bid_price_1 or tick.bid_price_1 == 0:
                tick.bid_price_1 = tick.last_price

            if not tick.ask_price_1 or tick.ask_price_1 == 0:
                tick.ask_price_1 = tick.last_price

            if not tick.bid_volume_1 or tick.bid_volume_1 == 0:
                tick.bid_volume_1 = 1

            if not tick.ask_volume_1 or tick.ask_volume_1 == 0:
                tick.ask_volume_1 = 1

            return tick

        except Exception:
            return None

    def detect_field_mapping(self, df: pd.DataFrame) -> Dict[str, str]:
        """æ£€æµ‹CSVå­—æ®µå¹¶è¿”å›æ˜ å°„"""
        detected_mapping = {}

        # å¸¸è§çš„ä¸­æ–‡å­—æ®µåæ˜ å°„
        chinese_mapping = {
            'æ—¶é—´': 'UpdateTime',
            'åˆçº¦ä»£ç ': 'InstrumentID',
            'æœ€æ–°ä»·': 'LastPrice',
            'æˆäº¤é‡': 'Volume',
            'æˆäº¤é¢': 'Turnover',
            'æŒä»“é‡': 'OpenInterest',
            'ä¹°ä¸€ä»·': 'BidPrice1',
            'ä¹°ä¸€é‡': 'BidVolume1',
            'å–ä¸€ä»·': 'AskPrice1',
            'å–ä¸€é‡': 'AskVolume1',
            'æ¶¨åœä»·': 'UpperLimitPrice',
            'è·Œåœä»·': 'LowerLimitPrice',
            'æ˜¨æ”¶': 'PreClosePrice',
            'å¼€ç›˜ä»·': 'OpenPrice',
            'æœ€é«˜ä»·': 'HighPrice',
            'æœ€ä½ä»·': 'LowPrice',
            'ç»“ç®—ä»·': 'SettlementPrice',
        }

        # é¦–å…ˆå°è¯•ä¸­æ–‡æ˜ å°„
        for csv_col in df.columns:
            if csv_col in chinese_mapping:
                standard_col = chinese_mapping[csv_col]
                if standard_col in self.TICK_FIELDS:
                    detected_mapping[standard_col] = self.TICK_FIELDS[standard_col]

        # ç„¶åå°è¯•ç›´æ¥åŒ¹é…æ ‡å‡†åˆ—å
        for csv_col in df.columns:
            if csv_col in self.TICK_FIELDS:
                detected_mapping[csv_col] = self.TICK_FIELDS[csv_col]

        return detected_mapping or self.TICK_FIELDS.copy()

    def import_file(self, file_path: Path, batch_size: int = 10000) -> Dict:
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        if not file_path.exists():
            return {'error': f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'file': str(file_path),
            'total_rows': 0,
            'valid_rows': 0,
            'invalid_rows': 0,
            'unique_symbols': set(),
            'saved_ticks': 0,
        }

        try:
            # è¯»å–CSV
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']
            df = None

            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if df is None:
                stats['error'] = "æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç "
                return stats

            stats['total_rows'] = len(df)

            # æ£€æµ‹å­—æ®µæ˜ å°„
            field_mapping = self.detect_field_mapping(df)

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'InstrumentID' not in df.columns or 'UpdateTime' not in df.columns:
                stats['error'] = "CSVç¼ºå°‘å¿…éœ€å­—æ®µ(InstrumentIDæˆ–UpdateTime)"
                return stats

            # è§£ææ•°æ®
            contract_ticks: Dict[str, List[TickData]] = {}

            for idx, row in df.iterrows():
                tick = self.parse_row_to_tick(row, idx, field_mapping)
                if tick:
                    stats['valid_rows'] += 1
                    stats['unique_symbols'].add(tick.symbol)

                    if tick.symbol not in contract_ticks:
                        contract_ticks[tick.symbol] = []
                    contract_ticks[tick.symbol].append(tick)
                else:
                    stats['invalid_rows'] += 1

            # ä¿å­˜æ•°æ®
            total_saved = 0
            for symbol, ticks in contract_ticks.items():
                # å»é‡
                unique_ticks = []
                seen = set()
                for tick in ticks:
                    key = (tick.symbol, tick.datetime)
                    if key not in seen:
                        seen.add(key)
                        unique_ticks.append(tick)

                # åˆ†æ‰¹ä¿å­˜
                for i in range(0, len(unique_ticks), batch_size):
                    batch = unique_ticks[i:i + batch_size]
                    try:
                        self.database.save_tick_data(batch)
                        total_saved += len(batch)
                    except Exception:
                        # å°è¯•é€æ¡ä¿å­˜
                        for tick in batch:
                            try:
                                self.database.save_tick_data([tick])
                                total_saved += 1
                            except Exception:
                                pass

            stats['saved_ticks'] = total_saved
            stats['unique_symbols'] = list(stats['unique_symbols'])

        except Exception as e:
            stats['error'] = str(e)

        return stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import sys

    parser = argparse.ArgumentParser(description='å¯¼å…¥CFFEXå¤šåˆçº¦Tickæ•°æ®åˆ°vn.pyæ•°æ®åº“')
    parser.add_argument('--path', type=str, required=True, help='CSVæ–‡ä»¶è·¯å¾„æˆ–åŒ…å«CSVæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=10000, help='æ‰¹å¤„ç†å¤§å°')

    args = parser.parse_args()

    try:
        # åˆ›å»ºå¯¼å…¥å™¨
        importer = CFFEXTickDataImporterFixed()

        path = Path(args.path)
        all_stats = []

        if path.is_file():
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            print(f"å¤„ç†æ–‡ä»¶: {path}")
            stats = importer.import_file(path, batch_size=args.batch_size)
            all_stats.append(stats)

        elif path.is_dir():
            # å¤„ç†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
            print(f"å¤„ç†æ–‡ä»¶å¤¹: {path}")
            csv_files = list(path.glob("*.csv"))
            if not csv_files:
                print(f"æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰CSVæ–‡ä»¶: {path}")
                sys.exit(1)

            print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

            for i, csv_file in enumerate(csv_files, 1):
                print(f"\n[{i}/{len(csv_files)}] å¤„ç†æ–‡ä»¶: {csv_file.name}")
                stats = importer.import_file(csv_file, batch_size=args.batch_size)
                all_stats.append(stats)

        else:
            print(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
            sys.exit(1)

        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š å¯¼å…¥æ±‡æ€»ç»Ÿè®¡")
        print("=" * 60)

        total_files = len(all_stats)
        successful_files = 0
        total_rows = 0
        total_valid = 0
        total_invalid = 0
        total_saved = 0
        all_symbols = set()

        for stats in all_stats:
            if 'error' in stats:
                print(f"âŒ {stats['file']}: {stats['error']}")
            else:
                successful_files += 1
                total_rows += stats['total_rows']
                total_valid += stats['valid_rows']
                total_invalid += stats['invalid_rows']
                total_saved += stats['saved_ticks']
                all_symbols.update(stats['unique_symbols'])

                print(f"âœ… {Path(stats['file']).name}: "
                      f"è¡Œæ•°:{stats['total_rows']}, "
                      f"æœ‰æ•ˆ:{stats['valid_rows']}, "
                      f"ä¿å­˜:{stats['saved_ticks']}, "
                      f"åˆçº¦:{len(stats['unique_symbols'])}")

        print(f"\næ€»è®¡: {successful_files}/{total_files} ä¸ªæ–‡ä»¶æˆåŠŸ")
        print(f"æ€»è¡Œæ•°: {total_rows}")
        print(f"æœ‰æ•ˆTickæ•°: {total_valid}")
        print(f"æ— æ•ˆè¡Œæ•°: {total_invalid}")
        print(f"ä¿å­˜Tickæ•°: {total_saved}")
        print(f"åˆçº¦åˆ—è¡¨: {sorted(all_symbols)}")
        print("=" * 60)

    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•:
    # python import_cffex_tick_data_fixed.py --path tick_data.csv (å•æ–‡ä»¶)
    # python import_cffex_tick_data_fixed.py --path ./tick_folder (æ–‡ä»¶å¤¹)
    main()